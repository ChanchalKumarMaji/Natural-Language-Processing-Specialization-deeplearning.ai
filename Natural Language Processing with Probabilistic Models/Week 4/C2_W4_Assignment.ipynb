{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Word Embeddings \n",
    "\n",
    "Welcome to the fourth programming assignment of Course 2. In this assignment we will show you how to compute the word embeddings. In Natural Language Processing (NLP) we can not only rely on counting the number of positive words and negative words, as we did in the last course using logistic regression. Instead we will try to find a way to represent each word by a vector. The vector could then represent syntactic (i.e. parts of speech) and semantic (i.e. meaning) structures. In this assignment you will explore a classic way of learning embeddings, or representations, of words by using a famous model called the continuous bag of words (CBOW) model. By completing this assignment you will:\n",
    "\n",
    "- Train word vectors from scratch.\n",
    "- Learn how to create batches of data.\n",
    "- Understand how backpropagation works.\n",
    "- Plot and visualize your learned word vectors.\n",
    "\n",
    "Because it will take a while to train your CBOW model, you will code the model and make sure to get the expected outputs. We will give you some slightly pre-trained vectors and then you will fine tune them on the Shakespeare dataset. \n",
    "\n",
    "Knowing how to train these models will give you a better understanding of word vectors, which are building blocks to many applications in natural language processing.\n",
    "\n",
    "\n",
    "# 1.0 The Continuous bag of words model\n",
    "\n",
    "Let's take a look at the following sentence: **'I am happy because I am learning'**. In continuous bag of words modeling we try to predict the center word given a few context words. For example, if you were to choose a context (say $C = 2$), then you would try to predict the word **happy** given the context: {I, am, because, I}. In other words, you have\n",
    "\n",
    "$$context = [I,am, because, I]$$\n",
    "$$target = happy$$\n",
    "\n",
    "The structure of your model will look like this:\n",
    "\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='word2.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:600px;height:250px;\" /> Figure 1 </div>\n",
    "\n",
    "Where $\\bar x$ is the average one hot vector for all the context word encodings. \n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='mean_vec2.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:600px;height:250px;\" /> Figure 2 </div>\n",
    "\n",
    "Once you have encoded all the context words, you can use $\\bar x$ as the input to your model. The architecture you will be implementing is as follows:\n",
    "\n",
    "\\begin{align}\n",
    " h &= W_1 \\  X + b_1  \\tag{1} \\\\\n",
    " a &= ReLU(h)  \\tag{2} \\\\\n",
    " z &= W_2 \\  a + b_2   \\tag{3} \\\\\n",
    " \\hat y &= softmax(z)   \\tag{4} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python libraries and helper functions (in utils2) \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from utils2 import sigmoid, get_batches, compute_pca, get_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sentence tokenizer\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 60996 \n",
      " ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n"
     ]
    }
   ],
   "source": [
    "# Load, tokenize and process the data\n",
    "import re                                                           #  Load the Regex-modul\n",
    "data = open('shakespeare.txt').read()                               #  Read in the data\n",
    "data = re.sub(r'[,!?;-]', '.',data)                                 #  Punktuations are replaced by .\n",
    "data = nltk.word_tokenize(data)                                     #  Tokenize string to words\n",
    "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\n",
    "print(\"Number of tokens:\", len(data),'\\n', data[:15])               #  print data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5778\n",
      "Most frequent tokens:  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1257), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 770), ('a', 752), ('you', 748), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n"
     ]
    }
   ],
   "source": [
    "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
    "fdist = nltk.FreqDist(word for word in data)\n",
    "print(\"Size of vocabulary: \",len(fdist) )\n",
    "print(\"Most frequent tokens: \",fdist.most_common(20) ) # print the 20 most frequent words and their freq."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping words to indices and indices to words\n",
    "We provide a helper function to create a dictionary that maps words to indices and indices to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5778\n"
     ]
    }
   ],
   "source": [
    "# get_dict creates two dictionaries, converting words to indices and viceversa.\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'king' :   2745\n",
      "Word which has index 2743:   kindness\n"
     ]
    }
   ],
   "source": [
    "# example of word to index mapping\n",
    "print(\"Index of the word 'king' :  \",word2Ind['king'] )\n",
    "print(\"Word which has index 2743:  \",Ind2word[2743] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Training the Model\n",
    "\n",
    "###  Initializing the model\n",
    "\n",
    "You will now initialize two matrices and two vectors. <br> The first matrix ($W_1$) is of dimension $V \\times N$, where $V$ is the number of words in your vocabulary and $N$ is the dimension of your word vector. <br>\n",
    "The second matrix ($W_2$) is of dimension $N \\times V$. \n",
    "The two vectors, $b_1$ and $b_2$ are of dimension $N\\times 1$ and $V\\times 1$ respectively (column vectors). $b_1$ and $b_2$ are the bias vectors of the linear layers from matrices $W_1$ and $W_2$.\n",
    "The overall structure of the model will look as in Figure 1, but at this stage we are just initializing the parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# GRADED FUNCTION: initialize_model\n",
    "def initialize_model(N,V):\n",
    "    ### START CODE HERE ###\n",
    "    W1 = np.random.random((N, V))\n",
    "    W2 = np.random.random((V, N))\n",
    "    b1 = np.random.random((N, 1))\n",
    "    b2 = np.random.random((V, 1))\n",
    "    ### END CODE HERE ###\n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10) (10, 4) [[0.88330609 0.62367221 0.75094243 0.34889834]]\n"
     ]
    }
   ],
   "source": [
    "# Test your function example.\n",
    "N = 4\n",
    "V = 10\n",
    "W1, W2, b1, b2 = initialize_model(N,V)\n",
    "assert W1.shape == ((N,V))\n",
    "assert W2.shape == ((V,N))\n",
    "print(W1.shape, W2.shape, b1.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Expected Output:**  (4, 10) (10, 4) [[0.88330609 0.62367221 0.75094243 0.34889834]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "Before we can start training the model, we need to implement the softmax function as defined in equation 5:  \n",
    "\n",
    "<br>\n",
    "$$ \\text{softmax}(z_i) = \\frac{e^{z_i} }{\\sum_{i} e^{z_i} }  \\tag{5} $$\n",
    "\n",
    "\n",
    "**Instructions**: Implement the softmax function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# GRADED FUNCTION: softmax\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR OWN CODE) ###\n",
    "    e_x = np.exp(x)\n",
    "    sum_e_x = np.sum(e_x)\n",
    "    s = e_x / sum_e_x\n",
    "    ### END CODE HERE ###\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04712342 0.94649912 0.00637746]\n"
     ]
    }
   ],
   "source": [
    "# testing the softmax function \n",
    "print(softmax([0,3,-2])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Ouput:**  array([0.04712342, 0.94649912, 0.00637746])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "Implement the forward propagation $z$ according to equations (1) to (3). <br>\n",
    "For that, you will use as activation the Rectified Linear Unit (ReLU) given by:\n",
    "\n",
    "$$f(h)=\\max (0,h) \\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# GRADED FUNCTION: forward_prop\n",
    "def forward_prop(x, W1, W2, b1, b2):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        W1, W2, b1, b2:  matrices and biases to be learned\n",
    "     Outputs: \n",
    "        z:  output score vector\n",
    "        h:  output of first hidden layer\n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR OWN CODE) ###\n",
    "    ### (YOU WILL NEED TO ADD CODE IN ADDITION TO JUST REPLACING THE 'None' VALUES) ###\n",
    "    \n",
    "    # Calculate h and then used it to calculate z\n",
    "    h = np.dot(W1, x) + b1\n",
    "    a = h.copy()\n",
    "    a[a < 0] = 0\n",
    "    z = np.dot(W2, a) + b2\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return z, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "We have implemented the *cross-entropy* cost function for you. <br>\n",
    "If you want to understand it better, we refer to a [good explanation](https://cs224d.stanford.edu/lecture_notes/notes1.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from scipy.misc import logsumexp\n",
    "except ImportError:\n",
    "    from scipy.special import logsumexp\n",
    "\n",
    "# compute_cost: cross-entropy cost function,\n",
    "def compute_cost(z, C, y, yhat, batch_size):\n",
    "    z_hat = logsumexp(z, axis=-1, keepdims=True)                      \n",
    "    cost = (-np.sum(y*np.log(yhat)) + np.sum(2.0*C*z_hat)) / batch_size\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training the Model\n",
    "\n",
    "Now that you have understood how the CBOW model works, you will train it. <br>\n",
    "You created a function for the forward propagation. Now you will implement a function that computes the gradients to backpropagate the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# GRADED FUNCTION: back_prop\n",
    "def back_prop(x, z, y, h, W1, W2, b1, b2, batch_size, m):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        z:  score vector\n",
    "        y:  target vector\n",
    "        h:  hidden vector (see eq. 1)\n",
    "        W1, W2, b1, b2:  matrices and biases  \n",
    "        batch_size: batch size \n",
    "        m:  number of context words\n",
    "     Outputs: \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n",
    "    '''\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR OWN CODE) ###\n",
    "    ### (YOU WILL NEED TO ADD CODE IN ADDITION TO JUST REPLACING THE 'None' VALUES) ###\n",
    "    def relu(k):\n",
    "        result = k.copy()\n",
    "        result[result < 0] = 0\n",
    "        return result\n",
    "    \n",
    "    yhat = softmax(z)\n",
    "    grad_W1 = np.dot(relu(np.dot(W2.T, yhat - y)), x.T)\n",
    "    grad_W2 = np.dot(yhat - y, h.T)\n",
    "    grad_b1 = relu(np.dot(W2.T, yhat - y))\n",
    "    grad_b2 = yhat - y\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have implemented a function to compute the gradients, you will implement batch gradient descent over your training set. \n",
    "\n",
    "**Hint:** For that, you will use initialize_model and the back_prop function that you just created (and the compute_cost function). You can also use the provided get_batches helper function:\n",
    "\n",
    "```for x, y in get_batches(data, word2Ind, V, C, batch_size):```\n",
    "\n",
    "```...```\n",
    "\n",
    "Also: print the cost after each batch is processed (use batch size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# GRADED FUNCTION: gradient_descent\n",
    "def gradient_descent(data, word2Ind, C, N, V, num_iters, alpha=0.03):\n",
    "    \n",
    "    '''\n",
    "    This is the gradient_descent function\n",
    "    \n",
    "      Inputs: \n",
    "        data:      text\n",
    "        word2Ind:  words to Indices\n",
    "        C:         context window\n",
    "        N:         dimension of hidden vector  \n",
    "        V:         dimension of vocabulary \n",
    "        num_iters: number of iterations  \n",
    "     Outputs: \n",
    "        W1, W2, b1, b2:  updated matrices and biases   \n",
    "\n",
    "    '''\n",
    "    W1, W2, b1, b2 = initialize_model(N,V)\n",
    "    m = (2*C)\n",
    "    batch_size = 128\n",
    "    iters = 0\n",
    "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR OWN CODE) ###\n",
    "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
    "        yhat = softmax(z)\n",
    "        cost = compute_cost(z, C, y, yhat, batch_size)\n",
    "        print('iters', iters + 1 , '   cost',cost)\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, z, y, h, W1, W2, b1, b2, batch_size, m)\n",
    "        W1 = W1 - alpha * grad_W1\n",
    "        W2 = W2 - alpha * grad_W2\n",
    "        b1 = b1 - alpha * grad_b1\n",
    "        b2 = b2 - alpha * grad_b2\n",
    "        iters += 1 \n",
    "        if iters == num_iters: \n",
    "            break\n",
    "        if iters % 100 == 0:\n",
    "            alpha *= 0.66\n",
    "    \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters 1    cost 5670.685215911188\n",
      "iters 2    cost 5664.065992365718\n",
      "iters 3    cost 5670.990718728146\n",
      "iters 4    cost 5677.9154450905735\n",
      "iters 5    cost nan\n",
      "iters 6    cost nan\n",
      "iters 7    cost nan\n",
      "iters 8    cost nan\n",
      "iters 9    cost nan\n",
      "iters 10    cost nan\n",
      "iters 11    cost nan\n",
      "iters 12    cost nan\n",
      "iters 13    cost nan\n",
      "iters 14    cost nan\n",
      "iters 15    cost nan\n"
     ]
    }
   ],
   "source": [
    "# test your function\n",
    "C = 2\n",
    "N = 50\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 15\n",
    "W1, W2, b1, b2 = gradient_descent(data, word2Ind, C, N, V, num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:** iters 15 cost 41.66082959286652"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Visualizing the word vectors\n",
    "\n",
    "In this part you will visualize the word vectors trained using the function you just coded above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50) [2745, 3951, 2961, 3023, 5675, 1452, 2472, 4191, 2316, 4278]\n"
     ]
    }
   ],
   "source": [
    "# visualizing the word vectors here\n",
    "from matplotlib import pyplot\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "words = ['king', 'queen','lord','man', 'woman','dog','horse',\n",
    "         'rich','happy','sad']\n",
    "\n",
    "embs = (W1.T + W2)/2.0\n",
    " \n",
    "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
    "idx = [word2Ind[word] for word in words]\n",
    "X = embs[idx, :]\n",
    "print(X.shape, idx)  # X.shape:  Number of words of dimension N each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bc7754381651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcompute_pca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/Week4/utils2.py\u001b[0m in \u001b[0;36mcompute_pca\u001b[0;34m(data, n_components)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# use 'eigh' rather than 'eig' since R is symmetric,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# the performance gain is substantial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;31m# sort eigenvalue in decreasing order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# this returns the corresponding indices of evals and evecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/linalg/decomp.py\u001b[0m in \u001b[0;36meigh\u001b[0;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected square matrix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m_asarray_validated\u001b[0;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'masked arrays are not supported'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AllFloat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         raise ValueError(\n\u001b[0;32m--> 498\u001b[0;31m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[1;32m    499\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "result= compute_pca(X, 2)\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that woman and queen are next to each other. However, we have to be carefull with the interpretation of this projected word vectors, since the PCA depends on the projection -- as shown in the following illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result= compute_pca(X, 4)\n",
    "pyplot.scatter(result[:, 3], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 3], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC2-4"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
