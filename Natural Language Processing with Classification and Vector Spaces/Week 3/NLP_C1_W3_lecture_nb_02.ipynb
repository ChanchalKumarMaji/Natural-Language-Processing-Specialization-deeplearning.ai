{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ungraded lab 2: Manipulating word embeddings \n",
    "\n",
    "*Copyrighted material*\n",
    "\n",
    "**Objectives:** Use numpy function to apply the most common linear algebra in Python\n",
    "\n",
    "By the end of this module you will be able to:\n",
    "* Use a pretrained word embedding to map words to vectors\n",
    "* Use a pretrained word embedding to map a sentence to a vector\n",
    "* Visualize word embeddings and identify its semantics by means of PCA to reduce its dimensionality\n",
    "* Create your own word embedding by training an Artificial Neural Network\n",
    "\n",
    "**From wikipedia:** \n",
    "\n",
    "\"_Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension.\n",
    "\n",
    "_Methods to generate this mapping include neural networks,[1] dimensionality reduction on the word co-occurrence matrix,[2][3][4] probabilistic models,[5] explainable knowledge base method,[6] and explicit representation in terms of the context in which words appear.[7]\n",
    "\n",
    "_Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing[8] and sentiment analysis.[9]_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "word_embeddings = pickle.load( open( \"word_embeddings_subset.p\", \"rb\" ) )\n",
    "len(word_embeddings) # there should be 243 words that will be used in this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is loaded, we can take a look to see how words are represented. First note that the _word_embeddings_ is a dictionary. Each word is the key of the tuple, and the value is its corresponding vector presentation. From previous labs, you should remember that you can access any entry in the dictionary by using square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryVector = word_embeddings['country'] # Get the vector representation for country\n",
    "print(type(countryVector)) # Print the type of the vector. Note it is a numpy array\n",
    "print(countryVector) # Print the values of the vector.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that each vector is stored as a numpy array. Don't forget it allows to use the linear algebra operations on it. \n",
    "\n",
    "The vectors have a size of 300 while the vocabulary size of google news is around 3 millions words! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the vector for a word:\n",
    "def vec(w):\n",
    "    return word_embeddings[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operating on word embeddings\n",
    "\n",
    "If word embedding are vector then you can operate over them using the linear algebra operators. But sometimes you need to have a visual representation of your data before you start using it.\n",
    "\n",
    "In the next cell, you will make a nice plot for the word embedding of some words. Even if plotting the dots gives you and idea of the words, the arrow representations helps to visualize how alligned are the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # Import matplotlib\n",
    "\n",
    "words = ['oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\n",
    "\n",
    "bag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n",
    "\n",
    "col1 = 3 # Select the column for the x axe\n",
    "col2 = 2 # Select the column for the y axe\n",
    "\n",
    "# Print an arrow for each word\n",
    "for word in bag2d:\n",
    "    ax.arrow(0, 0, word[col1], word[col2], head_width=0.005, head_length=0.005, fc='r', ec='r', width = 1e-5)\n",
    "\n",
    "    \n",
    "ax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n",
    "\n",
    "# Add the word label over each dot in the scatter plot\n",
    "for i in range(0, len(words)):\n",
    "    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that similar words like village, and town or petroleum, oil and gas, tends to point in the same direction. Also note that even if sad and happy looks to be close to each other the vectors points in opposite directions. \n",
    "\n",
    "In this chart you can figure it out the angles and distances between the words. Some words are close in both kinds of distance metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word distance\n",
    "\n",
    "Now let's plot the words 'sad', 'happy', 'town' and 'village'. Let's display the vector from village to town and the vector from sad to happy. Let's see that this can be done using linear algebra operations using numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['sad', 'happy', 'town', 'village']\n",
    "\n",
    "bag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n",
    "\n",
    "col1 = 3 # Select the column for the x axe\n",
    "col2 = 2 # Select the column for the y axe\n",
    "\n",
    "# Print an arrow for each word\n",
    "for word in bag2d:\n",
    "    ax.arrow(0, 0, word[col1], word[col2], head_width=0.0005, head_length=0.0005, fc='r', ec='r', width = 1e-5)\n",
    "    \n",
    "# print the vector difference between village and town\n",
    "village = vec('village')\n",
    "town = vec('town')\n",
    "diff = town - village\n",
    "ax.arrow(village[col1], village[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n",
    "\n",
    "# print the vector difference between village and town\n",
    "sad = vec('sad')\n",
    "happy = vec('happy')\n",
    "diff = happy - sad\n",
    "ax.arrow(sad[col1], sad[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n",
    "\n",
    "\n",
    "ax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n",
    "\n",
    "# Add the word label over each dot in the scatter plot\n",
    "for i in range(0, len(words)):\n",
    "    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear algebra on word embeddings\n",
    "\n",
    "As you observed during this week videos, you can perform some algebra on word embedding to find word analogies. Let's see how to do it in Python with Numpy.\n",
    "\n",
    "For example, you can get the \"norm\" of a given word in the embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.norm(vec('town')))\n",
    "print(np.linalg.norm(vec('sad')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting capitals\n",
    "\n",
    "Now, applying vector difference and addition, you can create a the vector representation for a new word. For example, we can say that the vector difference between France and Paris represents the concept of Capital.\n",
    "\n",
    "So, you can move translate from the city Madrid in the direction of the concept of Capital, and obtain something you would expect to be close to the corresponding country to which Madrid is the Capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital = vec('France') - vec('Paris')\n",
    "country = vec('Madrid') + capital\n",
    "\n",
    "print(country[0:5]) # Print the first 5 values of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the vector 'country' that we expect to be the same as the vector for Spain is not exactly it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = country - vec('Spain')\n",
    "print(diff[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you have to look for the closest words in your embedding that matches the candidate country. If your word embedding work as expected, the most similar word must be Spain. Let's define a function that help us to do it. We will create represent our word embedding as a DataFrame, which facilitate the lookup operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe out of the dictionary embedding. This facilitate the algebraic operations\n",
    "keys = word_embeddings.keys()\n",
    "data = []\n",
    "for key in keys:\n",
    "    data.append(word_embeddings[key])\n",
    "\n",
    "embedding = pd.DataFrame(data=data, index=keys)\n",
    "# Define a function to find the closest word to a vector:\n",
    "def find_closest_word(v, k = 1):\n",
    "    # Calculate the vector difference from each to word to the input vector\n",
    "    diff = embedding.values - v \n",
    "    # Get the norm of each difference vector. \n",
    "    # It means the squared euclidean distance from each word to the input vector\n",
    "    delta = np.sum(diff * diff, axis=1)\n",
    "    # Find the index of the minimun distance in the array\n",
    "    i = np.argmin(delta)\n",
    "    # Return the row name for this item\n",
    "    return embedding.iloc[i].name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some rows of the embedding as a Dataframe\n",
    "embedding.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the name that correspond to our numerical country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_word(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting other Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_word(vec('Italy') - vec('Rome') + vec('Madrid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_closest_word(vec('Berlin') + capital))\n",
    "print(find_closest_word(vec('Beijing') + capital))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it does not always work. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_closest_word(vec('Lisbon') + capital))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Represent a sentence as a vector\n",
    "\n",
    "You can represent a whole sentence as a vector by summing all the word vectors that conform the sentence. Let's see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"Spain petroleum city king\"\n",
    "vdoc = [vec(x) for x in doc.split(\" \")]\n",
    "doc2vec = np.sum(vdoc, axis = 0)\n",
    "doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_word(doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
