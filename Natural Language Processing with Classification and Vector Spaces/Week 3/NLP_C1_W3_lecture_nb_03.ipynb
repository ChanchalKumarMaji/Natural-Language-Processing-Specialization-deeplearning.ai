{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ungraded lab 3: Another explanation about PCA\n",
    "\n",
    "*Copyrighted material*\n",
    "\n",
    "**Objectives:** Apply PCA to solve many problems\n",
    "\n",
    "**Steps:**\n",
    "* PCA to find a rotation matrix\n",
    "* PCA to separate components of a mixture\n",
    "\n",
    "<img src = 'pca.jpeg' width=\"width\" height=\"height\"/>\n",
    "\n",
    "creds: Raunak Joshi\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\"Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.\n",
    "\n",
    "PCA was invented in 1901 by Karl Pearson,[1] as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s.[2] Depending on the field of application, it is also named the discrete Karhunen‚ÄìLo√®ve transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (Golub and Van Loan, 1983), eigenvalue decomposition (EVD) of XTX in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch. 7 of Jolliffe's Principal Component Analysis),[3] Eckart‚ÄìYoung theorem (Harman, 1960), or empirical orthogonal functions (EOF) in meteorological science, empirical eigenfunction decomposition (Sirovich, 1987), empirical component analysis (Lorenz, 1956), quasiharmonic modes (Brooks et al., 1988), spectral decomposition in noise and vibration, and empirical modal analysis in structural dynamics.\"\n",
    "\n",
    "<img src=GaussianScatterPCA.svg>\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Principal_component_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start lets consider a pair of random variables x, y. Lets start considering the base case when y = n * x. It is clear that x and y will be perfectly correlated to each other since y is just a scaling of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "x = np.random.uniform(1,2,1000)\n",
    "y = x.copy() * n\n",
    "\n",
    "# PCA works better if the data is centered\n",
    "x = x - np.mean(x)\n",
    "y = y - np.mean(y)\n",
    "\n",
    "data = pd.DataFrame({'x': x, 'y': y})\n",
    "plt.scatter(data.x, data.y)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pcaTr = pca.fit(data)\n",
    "dataPCA = pd.DataFrame(data = pcaTr.transform(data), columns = ['PC1', 'PC2'])\n",
    "\n",
    "print(pca.components_.T)\n",
    "\n",
    "plt.scatter(dataPCA.PC1, dataPCA.PC2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what is the direction in which the set of points are pointing the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undestanding the transformation model pcaTr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First Eigen Vector\n",
    "print('Eigen vectors or principal component: First row must be in the direction of [1, n]')\n",
    "print(pca.components_)\n",
    "\n",
    "# First Eigen Value\n",
    "print('Eigen values or explained variance')\n",
    "print(pca.explained_variance_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets introduce random variations to our correlated variables\n",
    "\n",
    "Now, we will use a controlled dataset composed of 2 random variables with different variances and with a certain covariance amogh them. The only way I know to get such a dataset\n",
    "is, first, create 2 independent random normal variables with the desired variances, and then combine them using a rotation matrix. In this way the new resulting variables will be a linear combination of the original random variables and thus be dependent and correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "\n",
    "std1 = 1\n",
    "std2 = 0.333\n",
    "\n",
    "x = np.random.normal(0,std1,1000)\n",
    "y = np.random.normal(0,std2,1000)\n",
    "#y = y + np.random.normal(0,1,1000)*noiseLevel * np.sin(0.78)\n",
    "\n",
    "# PCA works better if the data is centered\n",
    "x = x - np.mean(x)\n",
    "y = y - np.mean(y)\n",
    "\n",
    "#Define a pair of dependent variables with a desired amount of covariance\n",
    "n = 1 # Magnitude of covariance\n",
    "angle = -np.arctan(1/n)\n",
    "print('angle: ',  angle * 180 /math.pi)\n",
    "rotationMatrix = [[np.cos(angle), -np.sin(angle)],\n",
    "                 [np.sin(angle), np.cos(angle)]]\n",
    "\n",
    "\n",
    "print('rotationMatrix ', rotationMatrix)\n",
    "data = pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "print(data.iloc[0,:])\n",
    "#plt.scatter(data.x, data.y)\n",
    "# Rotation operation.\n",
    "data = data.dot(rotationMatrix)\n",
    "data.columns = ['x', 'y']\n",
    "print(data.iloc[0,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data.x, data.y)\n",
    "\n",
    "# Apply PCA. In theory, the Eigen Vector matrix must be the inverse of the original rotationMatrix. \n",
    "pca = PCA(n_components=2)\n",
    "pcaTr = pca.fit(data)\n",
    "dataPCA = pd.DataFrame(data = pcaTr.transform(data), columns = ['PC1', 'PC2'])\n",
    "# First Eigen Vector\n",
    "print('Eigen vectors or principal component: First row must be in the direction of [1, n]')\n",
    "print(pca.components_)\n",
    "\n",
    "# First Eigen Value\n",
    "print('Eigen values or explained variance')\n",
    "print(pca.explained_variance_)\n",
    "\n",
    "plt.scatter(dataPCA.PC1, dataPCA.PC2)\n",
    "plt.plot([0, rotationMatrix[0][0] * std1 * 3], [0, rotationMatrix[0][1] * std1 * 3], 'k-', color='red')\n",
    "plt.plot([0, rotationMatrix[1][0] * std2 * 3], [0, rotationMatrix[1][1] * std2 * 3], 'k-', color='green')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA as an strategy to reduce the dimensionality\n",
    "\n",
    "The principal components contained in the rotation matrix, are decreasingly sorted depending on its explained variance. It usually means that the first components retains most of the power of the data to explain the pattenrs that generalize the data. Nevertheless, for some applications, we are really interested in the patters that explain much less variance. For example in novelty detection. In the figure we can see the original data, and its corresponding proyection over the first and second principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPCA1 = pd.DataFrame(data = pcaTr.transform(data), columns = ['PC1', 'PC2'])\n",
    "\n",
    "plt.scatter(dataPCA.PC1, dataPCA.PC2)\n",
    "plt.scatter(dataPCA.PC1, np.zeros(len(dataPCA.PC1)))\n",
    "plt.scatter(np.zeros(len(dataPCA.PC2)), dataPCA.PC2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA as an strategy to plot complex data, as 2D or 3D points\n",
    "\n",
    "One of my favoury uses of PCA is to create 2D or 3D charts out of very complex or highly dimensional data. We can find a very nice example in our cats/dogs classification where use use PCA to plot in a 2D chart the set of images used for classification. Remember that images are composed of pixels and each image is contains hundreds, thousans or millions of them. So, be able to map those complex objects in a 2D space is just awesome. Follow the link to view the example: [https://github.com/andcastillo/classification/blob/master/images/Classification.ipynb]\n",
    "\n",
    "<img src = 'catdog.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still in progress. Sorry üòê "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
